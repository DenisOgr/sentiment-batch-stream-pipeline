{"cells": [{"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": "#!pip install pip install beautifulsoup4\n#!pip install nltk"}, {"cell_type": "code", "execution_count": 68, "metadata": {}, "outputs": [], "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.types import *\nimport re\nfrom pyspark.ml import Transformer, Pipeline\nfrom pyspark.sql.functions import udf\nfrom bs4 import BeautifulSoup\nfrom nltk.tokenize import WordPunctTokenizer\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.sql import Row\nfrom pyspark.ml.feature import NGram, Tokenizer, CountVectorizer, SQLTransformer\nimport pandas as pd\nimport pprint\npp = pprint.PrettyPrinter(indent=4)\npd.set_option('display.max_colwidth', 1000)\nspark = SparkSession.builder.getOrCreate()"}, {"cell_type": "code", "execution_count": 81, "metadata": {}, "outputs": [], "source": "schema = StructType([\n   StructField(\"sentiment\", IntegerType(), False),\n   StructField(\"id\", IntegerType(), False),\n   StructField(\"date\", StringType(), False),\n   StructField(\"query_string\", StringType(), False),\n   StructField(\"user\", StringType(), False),\n   StructField(\"text\", StringType(), False),\n])\npath = \"gs://sentiment-twitter-analys/data/trainingandtestdata/training.1600000.processed.noemoticon.csv\"\ndf = spark.read.format(\"csv\").load(path, schema=schema)"}, {"cell_type": "code", "execution_count": 82, "metadata": {}, "outputs": [], "source": "df = df.select(['sentiment', 'text']).limit(100000)"}, {"cell_type": "markdown", "metadata": {}, "source": "### PreProcessing"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": "pat1 = r'@[A-Za-z0-9_]+'\npat2 = r'https?://[^ ]+'\ncombined_pat = r'|'.join((pat1, pat2))\nwww_pat = r'www.[^ ]+'\nnegations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n                \"mustn't\":\"must not\"}\nneg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n\n"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": "\nclass PreprocessingTokenizer(Transformer):\n\n    pat1 = r'@[A-Za-z0-9_]+'\n    pat2 = r'https?://[^ ]+'\n    combined_pat = r'|'.join((pat1, pat2))\n    www_pat = r'www.[^ ]+'\n    negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n                    \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n                    \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n                    \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n                    \"mustn't\":\"must not\"}\n    neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n    tok = WordPunctTokenizer()\n    \n    def __init__(self, inputCol=None, outputCol=None):\n        super(PreprocessingTokenizer, self).__init__()\n        self.inputCol=inputCol\n        self.outputCol=outputCol\n        \n       \n\n    def _transform(self, dataset):\n        def f(text):\n            soup = BeautifulSoup(text, 'lxml')\n            souped = soup.get_text()\n            try:\n                bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n            except:\n                bom_removed = souped\n            stripped = re.sub(self.combined_pat, '', bom_removed)\n            stripped = re.sub(self.www_pat, '', stripped)\n            lower_case = stripped.lower()\n            neg_handled = self.neg_pattern.sub(lambda x: self.negations_dic[x.group()], lower_case)\n            letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n            words = [x for x  in self.tok.tokenize(letters_only) if len(x) > 1]\n            return (\" \".join(words)).strip()\n\n        return dataset.withColumn(self.outputCol, udf(f, StringType())(dataset[self.inputCol]))"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": "pt = PreprocessingTokenizer(inputCol='text', outputCol='text_clean')"}, {"cell_type": "markdown", "metadata": {}, "source": "### Feature engineering"}, {"cell_type": "code", "execution_count": 83, "metadata": {}, "outputs": [], "source": "tk = Tokenizer(inputCol='text', outputCol='words')\nng1 = NGram(n=1, inputCol='words', outputCol='1_gr_words')\nng2 = NGram(n=2, inputCol='words', outputCol='2_gr_words')\nng3 = NGram(n=3, inputCol='words', outputCol='3_gr_words')\nstatement = \"\"\"\nSELECT\n    *, concat(1_gr_words, 2_gr_words, 3_gr_words) c_words\nFROM\n    __THIS__\n\"\"\"\nsql = SQLTransformer(statement=statement)\ncv = CountVectorizer(inputCol='c_words', vocabSize=80000, outputCol='features')"}, {"cell_type": "markdown", "metadata": {}, "source": "### Building model"}, {"cell_type": "code", "execution_count": 84, "metadata": {}, "outputs": [], "source": "df_train, df_test = df.randomSplit([0.8, 0.2], seed=100500)"}, {"cell_type": "code", "execution_count": 85, "metadata": {}, "outputs": [{"data": {"text/plain": "(79872, 20128)"}, "execution_count": 85, "metadata": {}, "output_type": "execute_result"}], "source": "df_train.count(), df_test.count()"}, {"cell_type": "code", "execution_count": 86, "metadata": {}, "outputs": [], "source": "lr = LogisticRegression(featuresCol='features', labelCol='sentiment', maxIter=5000)\npipeline_model = Pipeline(stages=[pt, tk, ng1, ng2, ng3, sql, cv, lr]).fit(df_train)"}, {"cell_type": "markdown", "metadata": {}, "source": "### Evaluating model"}, {"cell_type": "code", "execution_count": 87, "metadata": {}, "outputs": [], "source": "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"}, {"cell_type": "code", "execution_count": 88, "metadata": {}, "outputs": [], "source": "ev = MulticlassClassificationEvaluator(labelCol='sentiment',metricName=\"accuracy\", predictionCol='prediction')"}, {"cell_type": "code", "execution_count": 89, "metadata": {}, "outputs": [], "source": "df_predict = pipeline_model.transform(df_test).cache()"}, {"cell_type": "code", "execution_count": 90, "metadata": {}, "outputs": [{"data": {"text/plain": "1.0"}, "execution_count": 90, "metadata": {}, "output_type": "execute_result"}], "source": "ev.evaluate(df_predict)"}, {"cell_type": "code", "execution_count": 92, "metadata": {}, "outputs": [{"data": {"text/plain": "0"}, "execution_count": 92, "metadata": {}, "output_type": "execute_result"}], "source": "df_predict.select('sentiment').where('sentiment==0').count()"}, {"cell_type": "markdown", "metadata": {}, "source": "### Store model(pipeline)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 2}